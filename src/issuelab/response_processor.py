"""
Agent Response ÂêéÂ§ÑÁêÜÔºöËß£Êûê @mentions Âπ∂Ëß¶Âèë dispatch

Ëß£ÂÜ≥botËØÑËÆ∫Êó†Ê≥ïËß¶ÂèëworkflowÁöÑÈóÆÈ¢òÔºö
- agentÊâßË°åÂÆåÊàêÂêé‰∏ªÂä®Ëß£Êûêresponse‰∏≠ÁöÑ@mentions
- Ëá™Âä®Ëß¶ÂèëË¢´@ÁöÑÁî®Êà∑agent
"""

import logging
import os
import re
import subprocess
from pathlib import Path
from typing import Any, cast

import yaml

from issuelab.mention_policy import (
    build_mention_section,
    clean_mentions_in_text,
    filter_mentions,
)
from issuelab.utils.yaml_text import extract_yaml_block

logger = logging.getLogger(__name__)

__all__ = [
    "build_mention_section",
    "clean_mentions_in_text",
    "filter_mentions",
    "trigger_mentioned_agents",
    "process_agent_response",
    "normalize_comment_body",
    "extract_mentions_from_yaml",
    "should_auto_close",
    "close_issue",
    "get_mentions_max_count",
]


_DEFAULT_FORMAT_RULES = {
    "force_normalize": False,
    "sections": {
        "summary": "## Summary",
        "findings": "## Key Findings",
        "actions": "## Recommended Actions",
        "sources": "## Sources",
        "structured": "## Structured (YAML)",
    },
    "limits": {
        "summary_max_chars": 20,
        "findings_count": 3,
        "findings_max_chars": 25,
        "actions_max_count": 2,
        "actions_max_chars": 30,
        "mentions_max_count": 5,
    },
    "rules": {
        "mentions_only_in_actions": True,
        "yaml_required": True,
    },
}

_FORMAT_RULES_CACHE: dict[str, Any] | None = None


def _find_marker(response_text: str, candidates: list[str], canonical: str) -> tuple[int, str]:
    """Return the earliest matched marker and its text."""
    best_pos = -1
    best_marker = canonical
    for marker in candidates:
        pos = response_text.find(marker)
        if pos == -1:
            continue
        if best_pos == -1 or pos < best_pos:
            best_pos = pos
            best_marker = marker
    return best_pos, best_marker


def _remove_last_yaml_fence(text: str) -> str:
    """Remove the last ```yaml fenced block from text."""
    matches = list(re.finditer(r"```yaml[\s\S]*?```", text, re.IGNORECASE))
    if not matches:
        return text
    start, end = matches[-1].span()
    return text[:start] + text[end:]


def _load_format_rules() -> dict[str, Any]:
    global _FORMAT_RULES_CACHE
    if _FORMAT_RULES_CACHE is not None:
        return _FORMAT_RULES_CACHE

    rules = cast(dict[str, Any], {**_DEFAULT_FORMAT_RULES})
    config_path = Path(__file__).resolve().parents[2] / "config" / "response_format.yml"
    if config_path.exists():
        try:
            with config_path.open("r", encoding="utf-8") as handle:
                data = yaml.safe_load(handle) or {}
            rules.update({k: v for k, v in data.items() if k in rules})
            sections = data.get("sections", {})
            if isinstance(sections, dict):
                cast(dict[str, Any], rules.get("sections", {})).update(sections)
            limits = data.get("limits", {})
            if isinstance(limits, dict):
                cast(dict[str, Any], rules.get("limits", {})).update(limits)
            rule_items = data.get("rules", {})
            if isinstance(rule_items, dict):
                cast(dict[str, Any], rules.get("rules", {})).update(rule_items)
        except Exception as exc:
            logger.warning("Failed to load response format rules: %s", exc)

    _FORMAT_RULES_CACHE = rules
    return rules


def _truncate_text(text: str, limit: int) -> str:
    text = " ".join(text.split())
    if limit <= 0:
        return ""
    if len(text) <= limit:
        return text
    return text[:limit]


def get_mentions_max_count(default: int = 5) -> int:
    """Get mention cap from response format rules."""
    rules = _load_format_rules()
    limits = rules.get("limits", {})
    try:
        value = int(limits.get("mentions_max_count", default))
        return value if value > 0 else default
    except Exception:
        return default


def _extract_sources_from_parsed_yaml(parsed: Any) -> list[str]:
    if not isinstance(parsed, dict):
        return []
    raw_sources = parsed.get("sources", [])
    urls: list[str] = []
    if isinstance(raw_sources, list):
        for item in raw_sources:
            if isinstance(item, str):
                urls.append(item.strip())
            elif isinstance(item, dict):
                url = str(item.get("url", "")).strip()
                if url:
                    urls.append(url)
    elif isinstance(raw_sources, str):
        urls.append(raw_sources.strip())

    normalized: list[str] = []
    for url in urls:
        if not url:
            continue
        if not re.match(r"^https?://", url, re.IGNORECASE):
            continue
        if url not in normalized:
            normalized.append(url)
    return normalized


def extract_mentions_from_yaml(response_text: str) -> list[str]:
    yaml_text = extract_yaml_block(response_text)
    if not yaml_text:
        return []
    try:
        parsed = yaml.safe_load(yaml_text) or {}
    except Exception:
        return []
    mentions_value = parsed.get("mentions")
    if not mentions_value:
        return []

    if isinstance(mentions_value, str):
        candidates = [mentions_value]
    elif isinstance(mentions_value, list):
        candidates = [str(item) for item in mentions_value]
    else:
        return []

    normalized: list[str] = []
    for item in candidates:
        cleaned = item.strip()
        if cleaned.startswith("@"):
            cleaned = cleaned[1:]
        if not cleaned:
            continue
        if not re.fullmatch(r"[a-zA-Z0-9_-]+", cleaned):
            continue
        if cleaned not in normalized:
            normalized.append(cleaned)
    return normalized


def _normalize_agent_output(response_text: str, agent_name: str | None) -> tuple[str, list[str]]:
    warnings: list[str] = []
    rules = _load_format_rules()
    sections = rules["sections"]
    limits = rules["limits"]
    force_normalize = bool(rules.get("force_normalize", False))

    summary_marker = sections["summary"]
    findings_marker = sections["findings"]
    actions_marker = sections["actions"]
    sources_marker = sections.get("sources", "## Sources")
    yaml_marker = sections["structured"]
    actions_aliases = [actions_marker, "## Recommendations"]
    yaml_aliases = [yaml_marker, "## Confidence"]

    # Render YAML-only responses into markdown for user-facing agents.
    if summary_marker not in response_text:
        yaml_text = extract_yaml_block(response_text)
        if yaml_text and agent_name not in {"arxiv_observer", "pubmed_observer"}:
            try:
                parsed = yaml.safe_load(yaml_text) or {}
            except Exception:
                parsed = None
            if isinstance(parsed, dict):
                summary_line = str(parsed.get("summary", "")).strip()
                findings_list = parsed.get("findings", []) if isinstance(parsed.get("findings"), list) else []
                recs_list = parsed.get("recommendations", []) if isinstance(parsed.get("recommendations"), list) else []
                sources_list = _extract_sources_from_parsed_yaml(parsed)

                findings_count = int(limits.get("findings_count", 3))
                actions_max = int(limits.get("actions_max_count", 2))
                summary_line = _truncate_text(
                    clean_mentions_in_text(summary_line), int(limits.get("summary_max_chars", 20))
                )
                findings = [
                    _truncate_text(clean_mentions_in_text(str(item)), int(limits.get("findings_max_chars", 25)))
                    for item in findings_list[:findings_count]
                ]
                actions = [
                    _truncate_text(str(item), int(limits.get("actions_max_chars", 30)))
                    for item in recs_list[:actions_max]
                ]

                normalized = [
                    f"[Agent: {agent_name}]",
                    "",
                    summary_marker,
                    summary_line or "(missing)",
                    "",
                    findings_marker,
                    *(f"- {item}" for item in findings),
                    "",
                    actions_marker,
                    *(f"- [ ] {item}" for item in actions),
                    "",
                    sources_marker,
                    *(f"- {item}" for item in sources_list),
                    "",
                    yaml_marker,
                    "```yaml",
                    yaml_text.strip(),
                    "```",
                ]

                return "\n".join(normalized).rstrip() + "\n", warnings

    if not force_normalize and summary_marker not in response_text:
        return response_text, warnings

    summary_pos, summary_used = _find_marker(response_text, [summary_marker], summary_marker)
    findings_pos, findings_used = _find_marker(response_text, [findings_marker], findings_marker)
    actions_pos, actions_used = _find_marker(response_text, actions_aliases, actions_marker)
    yaml_pos, yaml_used = _find_marker(response_text, yaml_aliases, yaml_marker)

    positions = {
        summary_marker: summary_pos,
        findings_marker: findings_pos,
        actions_marker: actions_pos,
        yaml_marker: yaml_pos,
    }
    used_markers = {
        summary_marker: summary_used,
        findings_marker: findings_used,
        actions_marker: actions_used,
        yaml_marker: yaml_used,
    }
    missing = [marker for marker, pos in positions.items() if pos == -1]
    if missing:
        warnings.append(f"Missing sections: {', '.join(missing)}")
        if not force_normalize:
            return response_text, warnings

    summary_block = response_text[
        positions.get(summary_marker, 0) + len(used_markers.get(summary_marker, summary_marker)) : positions.get(
            findings_marker, len(response_text)
        )
    ].strip()
    findings_block = response_text[
        positions.get(findings_marker, len(response_text))
        + len(used_markers.get(findings_marker, findings_marker)) : positions.get(actions_marker, len(response_text))
    ].strip()
    actions_block = response_text[
        positions.get(actions_marker, len(response_text))
        + len(used_markers.get(actions_marker, actions_marker)) : positions.get(yaml_marker, len(response_text))
    ].strip()
    yaml_block = response_text[
        positions.get(yaml_marker, len(response_text)) + len(used_markers.get(yaml_marker, yaml_marker)) :
    ].strip()

    summary_line = ""
    for line in summary_block.splitlines():
        if line.strip():
            summary_line = line.strip()
            break
    if not summary_line:
        warnings.append("Summary is empty")
    summary_line = clean_mentions_in_text(summary_line)
    summary_line = _truncate_text(summary_line, int(limits.get("summary_max_chars", 20)))

    findings_items: list[str] = []
    for line in findings_block.splitlines():
        match = re.match(r"^\s*[-*]\s+(.*)", line)
        if match:
            findings_items.append(match.group(1).strip())
    if not findings_items:
        warnings.append("Key Findings missing bullets")
    findings_count = int(limits.get("findings_count", 3))
    findings_items = [
        _truncate_text(clean_mentions_in_text(item), int(limits.get("findings_max_chars", 25)))
        for item in findings_items[:findings_count]
    ]
    if len(findings_items) < findings_count:
        warnings.append(f"Key Findings fewer than {findings_count} bullets")

    actions_items: list[str] = []
    for line in actions_block.splitlines():
        match = re.match(r"^\s*[-*]\s+(.*)", line)
        if match:
            actions_items.append(match.group(1).strip())
    if not actions_items:
        warnings.append("Recommended Actions missing bullets")
    actions_max = int(limits.get("actions_max_count", 2))
    actions_items = [
        _truncate_text(item, int(limits.get("actions_max_chars", 30))) for item in actions_items[:actions_max]
    ]
    if len(actions_items) > actions_max:
        warnings.append(f"Recommended Actions truncated to {actions_max} bullets")

    confidence = "medium"
    parsed_mentions: list[str] = []
    parsed_sources: list[str] = []
    yaml_text = extract_yaml_block(yaml_block)
    parsed = None
    if yaml_text:
        try:
            parsed = yaml.safe_load(yaml_text)
        except Exception:
            parsed = None
        if isinstance(parsed, dict):
            parsed_confidence = str(parsed.get("confidence", "")).lower()
            if parsed_confidence in {"high", "medium", "low"}:
                confidence = parsed_confidence
            parsed_mentions = extract_mentions_from_yaml(response_text)
            parsed_sources = _extract_sources_from_parsed_yaml(parsed)

    def _yaml_escape(value: str) -> str:
        return value.replace('"', '\\"')

    yaml_lines = [
        "```yaml",
        f'summary: "{_yaml_escape(summary_line)}"',
        "findings:",
    ]
    for item in findings_items:
        yaml_lines.append(f'  - "{_yaml_escape(item)}"')
    yaml_lines.append("recommendations:")
    for item in actions_items:
        yaml_lines.append(f'  - "{_yaml_escape(item)}"')
    if parsed_mentions:
        mentions_max = get_mentions_max_count(default=5)
        yaml_lines.append("mentions:")
        for item in parsed_mentions[:mentions_max]:
            yaml_lines.append(f'  - "{_yaml_escape(item)}"')
    yaml_lines.append(f'confidence: "{confidence}"')
    yaml_lines.append("```")

    normalized_lines: list[str] = []
    if agent_name:
        normalized_lines.extend([f"[Agent: {agent_name}]", ""])
    normalized_lines.extend(
        [
            summary_marker,
            summary_line or "(missing)",
            "",
            findings_marker,
            *(f"- {item}" for item in findings_items),
            "",
            actions_marker,
            *(f"- [ ] {item}" for item in actions_items),
            "",
            sources_marker,
            *(f"- {item}" for item in parsed_sources),
            "",
            yaml_marker,
            *yaml_lines,
        ]
    )

    return "\n".join(normalized_lines).rstrip() + "\n", warnings


def _extract_agent_name(response_text: str) -> str:
    lines = response_text.strip().splitlines()
    first_line = lines[0] if lines else ""
    match = re.search(r"^\[Agent:\s*(.+?)\]\s*$", first_line)
    if match:
        return match.group(1).strip()
    inline = re.search(r"\[Agent:\s*(.+?)\]", response_text)
    if inline:
        return inline.group(1).strip()
    return "unknown"


def normalize_comment_body(body: str, agent_name: str | None = None) -> str:
    if not body:
        return body
    if not agent_name:
        inferred = _extract_agent_name(body)
        agent_name = inferred if inferred != "unknown" else None
    normalized, _warnings = _normalize_agent_output(body, agent_name)
    rules = _load_format_rules()
    yaml_marker = rules["sections"]["structured"]
    if yaml_marker in normalized:
        head, _sep, _tail = normalized.partition(yaml_marker)
        normalized = head.rstrip() + "\n"
    else:
        normalized = _remove_last_yaml_fence(normalized)
    normalized = re.sub(r"\n{3,}", "\n\n", normalized).strip() + "\n"
    return normalized


def trigger_mentioned_agents(
    response: str,
    issue_number: int,
    issue_title: str,
    issue_body: str,
    policy: dict | None = None,
) -> tuple[dict[str, bool], list[str], list[str]]:
    """
    Ëß£Êûêagent response‰∏≠ÁöÑ@mentionsÔºåÂ∫îÁî®Á≠ñÁï•ËøáÊª§ÔºåÂπ∂Ëß¶ÂèëÂÖÅËÆ∏ÁöÑagent

    Args:
        response: AgentÁöÑresponseÂÜÖÂÆπ
        issue_number: IssueÁºñÂè∑
        issue_title: IssueÊ†áÈ¢ò
        issue_body: IssueÂÜÖÂÆπ
        policy: @ Á≠ñÁï•ÈÖçÁΩÆÔºàNone ÂàôËá™Âä®Âä†ËΩΩÔºâ

    Returns:
        (results, allowed_mentions, filtered_mentions)
    """
    mentions = extract_mentions_from_yaml(response)

    if not mentions:
        logger.info("[INFO] Response‰∏≠Ê≤°Êúâ@mentions")
        return {}, [], []

    logger.info(f"[INFO] ÂèëÁé∞ {len(mentions)} ‰∏™@mentions: {mentions}")

    # Â∫îÁî®Á≠ñÁï•ËøáÊª§
    allowed_mentions, filtered_mentions = filter_mentions(mentions, policy, issue_number=issue_number)

    if filtered_mentions:
        logger.info(f"[FILTER] ËøáÊª§‰∫Ü {len(filtered_mentions)} ‰∏™@mentions: {filtered_mentions}")

    if not allowed_mentions:
        logger.info("[INFO] Ê≤°ÊúâÂÖÅËÆ∏ÁöÑ@mentions")
        return {}, [], filtered_mentions

    logger.info(f"[INFO] ÂÖÅËÆ∏Ëß¶Âèë {len(allowed_mentions)} ‰∏™@mentions: {allowed_mentions}")

    from issuelab.observer_trigger import auto_trigger_agent

    results = {}
    for username in allowed_mentions:
        logger.info(f"[INFO] Ëß¶ÂèëË¢´@ÁöÑagent: {username}")
        success = auto_trigger_agent(
            agent_name=username,
            issue_number=issue_number,
            issue_title=issue_title,
            issue_body=issue_body,
        )
        results[username] = success

        if success:
            logger.info(f"[OK] ÊàêÂäüËß¶Âèë {username}")
        else:
            logger.error(f"[ERROR] Ëß¶Âèë {username} Â§±Ë¥•")

    return results, allowed_mentions, filtered_mentions


def process_agent_response(
    agent_name: str,
    response: str | dict[str, Any],
    issue_number: int,
    issue_title: str = "",
    issue_body: str = "",
    auto_dispatch: bool = True,
) -> dict[str, Any]:
    """
    Â§ÑÁêÜagent responseÁöÑÂêéÂ§ÑÁêÜÈÄªËæë

    Êñ∞Â¢ûÂäüËÉΩÔºö
    1. Ê∏ÖÁêÜ‰∏ª‰ΩìÂÜÖÂÆπ‰∏≠ÁöÑÊâÄÊúâ @mentionsÔºàÊõøÊç¢‰∏∫"Áî®Êà∑ xxx"Ôºâ
    2. Â∫îÁî®Á≠ñÁï•ËøáÊª§ÔºåÂå∫ÂàÜÂÖÅËÆ∏ÂíåË¢´ËøáÊª§ÁöÑ mentions
    3. Ëß¶ÂèëÂÖÅËÆ∏ÁöÑ agents
    4. ËøîÂõûÊ∏ÖÁêÜÂêéÁöÑ‰∏ª‰ΩìÂÜÖÂÆπÂíå mentions ‰ø°ÊÅØ

    Args:
        agent_name: AgentÂêçÁß∞
        response: AgentÁöÑresponseÔºàÂ≠óÁ¨¶‰∏≤ÊàñdictÔºâ
        issue_number: IssueÁºñÂè∑
        issue_title: IssueÊ†áÈ¢ò
        issue_body: IssueÂÜÖÂÆπ
        auto_dispatch: ÊòØÂê¶Ëá™Âä®Ëß¶Âèë@mentions

    Returns:
        Â§ÑÁêÜÁªìÊûú {
            "agent_name": str,
            "response": str,  # ÂéüÂßãÂõûÂ§ç
            "clean_response": str,  # Ê∏ÖÁêÜÂêéÁöÑÂõûÂ§çÔºàÊâÄÊúâ @ ÊõøÊç¢‰∏∫"Áî®Êà∑ xxx"Ôºâ
            "mentions": list[str],  # ÊâÄÊúâÊèêÂèñÁöÑ mentions
            "allowed_mentions": list[str],  # ÂÖÅËÆ∏ÁöÑ mentions
            "filtered_mentions": list[str],  # Ë¢´ËøáÊª§ÁöÑ mentions
            "dispatch_results": dict[str, bool]  # Ëß¶ÂèëÁªìÊûú
        }
    """
    # ÊèêÂèñresponseÊñáÊú¨
    response_text = response.get("response", str(response)) if isinstance(response, dict) else str(response)
    raw_response_text = response_text

    normalized_response, format_warnings = _normalize_agent_output(response_text, agent_name)
    if agent_name == "gqy20":
        yaml_text = extract_yaml_block(response_text)
        parsed = None
        if yaml_text:
            try:
                parsed = yaml.safe_load(yaml_text)
            except Exception:
                parsed = None
        sources = _extract_sources_from_parsed_yaml(parsed)
        if not sources:
            format_warnings.append("Missing required sources URLs for gqy20")
    if format_warnings:
        logger.warning("Response format warnings for '%s': %s", agent_name, "; ".join(format_warnings))
    response_text = normalized_response

    # ÊèêÂèñÊâÄÊúâ @mentionsÔºàÂü∫‰∫éÂéüÂßãÂõûÂ§çÔºåÈÅøÂÖçËßÑËåÉÂåñÂêé‰∏¢Â§±Ôºâ
    mentions = extract_mentions_from_yaml(raw_response_text)

    # Ê∏ÖÁêÜ‰∏ª‰ΩìÂÜÖÂÆπÔºàÂ∞ÜÊâÄÊúâ @username ÊõøÊç¢‰∏∫ "Áî®Êà∑ username"Ôºâ
    clean_response = clean_mentions_in_text(response_text)

    result: dict[str, Any] = {
        "agent_name": agent_name,
        "response": response_text,
        "raw_response": raw_response_text,
        "clean_response": clean_response,
        "mentions": mentions,
        "allowed_mentions": [],
        "filtered_mentions": [],
        "dispatch_results": {},
        "format_warnings": format_warnings,
    }

    # Ëá™Âä®Ëß¶ÂèëË¢´@ÁöÑagents
    if auto_dispatch and mentions:
        logger.info(f"üîó {agent_name} ÁöÑresponse‰∏≠@‰∫Ü {len(mentions)} ‰∏™Áî®Êà∑")
        dispatch_results, allowed_mentions, filtered_mentions = trigger_mentioned_agents(
            response_text, issue_number, issue_title, issue_body
        )
        result["dispatch_results"] = dispatch_results
        result["allowed_mentions"] = allowed_mentions
        result["filtered_mentions"] = filtered_mentions

    return result


def should_auto_close(response_text: str, agent_name: str) -> bool:
    """
    Ê£ÄÊü•ÊòØÂê¶Â∫îËØ•Ëá™Âä®ÂÖ≥Èó≠Issue

    ËßÑÂàôÔºö
    - ‰ªÖÈôê summarizer ÂèØËß¶ÂèëËá™Âä®ÂÖ≥Èó≠
    - ÂìçÂ∫î‰∏≠ÂøÖÈ°ªÂåÖÂê´ [CLOSE] Ê†áËÆ∞

    Args:
        response_text: AgentÁöÑresponseÂÜÖÂÆπ
        agent_name: AgentÂêçÁß∞

    Returns:
        ÊòØÂê¶Â∫îËØ•ÂÖ≥Èó≠
    """
    if agent_name != "summarizer":
        return False

    if not response_text:
        return False

    # Ê£ÄÊµã [CLOSE] Ê†áËÆ∞
    return "[CLOSE]" in response_text


def close_issue(issue_number: int) -> bool:
    """
    ÂÖ≥Èó≠ Issue

    Args:
        issue_number: IssueÁºñÂè∑

    Returns:
        ÊòØÂê¶ÊàêÂäüÂÖ≥Èó≠
    """
    try:
        result = subprocess.run(
            [
                "gh",
                "issue",
                "close",
                str(issue_number),
                "--repo",
                os.environ.get("GITHUB_REPOSITORY", ""),
                "--reason",
                "completed",
            ],
            capture_output=True,
            text=True,
            env=os.environ.copy(),
        )
        if result.returncode == 0:
            logger.info(f"[OK] Issue #{issue_number} Â∑≤Ëá™Âä®ÂÖ≥Èó≠")
            return True
        else:
            logger.error(f"[ERROR] ÂÖ≥Èó≠ Issue #{issue_number} Â§±Ë¥•: {result.stderr}")
            return False
    except Exception as e:
        logger.error(f"[ERROR] ÂÖ≥Èó≠ Issue #{issue_number} ÂºÇÂ∏∏: {e}")
        return False

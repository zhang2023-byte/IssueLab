#!/usr/bin/env python3
"""
arXiv è®ºæ–‡ç›‘æ§è„šæœ¬

å®šæœŸæ‰«æ arXiv æ–°è®ºæ–‡ï¼Œè¿”å›æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–°è®ºæ–‡åˆ—è¡¨ã€‚

Usage:
    python scripts/monitor_arxiv.py \
        --categories "cs.AI,cs.LG,cs.CL" \
        --last-scan "2026-01-01T00:00:00Z" \
        --max-papers 10 \
        --output /tmp/papers.json
"""

import argparse
import json
import re
import sys
import urllib.parse
from datetime import datetime
from typing import Any

import feedparser


def parse_arxiv_date(date_str: str) -> str:
    """è§£æ arXiv æ—¥æœŸæ ¼å¼ä¸º ISO æ ¼å¼"""
    try:
        # arXiv æ—¥æœŸæ ¼å¼: "2026-01-15T20:00:00Z"
        dt = datetime.strptime(date_str[:19], "%Y-%m-%dT%H:%M:%S")
        return dt.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        return date_str[:10] if date_str else "Unknown"


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™ç©ºç™½"""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def truncate_text(text: str, max_length: int = 1500) -> str:
    """æˆªæ–­æ–‡æœ¬ï¼Œä¿ç•™æ‘˜è¦å¯ç”¨"""
    if len(text) <= max_length:
        return text
    return text[:max_length].rsplit(".", 1)[0] +..."


def fetch_arxiv_papers(
    categories: list[str],
    last_scan: str,
    max_papers: int = 10,
) -> list[dict[str, Any]]:
    """
    è·å–æŒ‡å®šåˆ†ç±»ä¸‹çš„æ–°è®ºæ–‡

    Args:
        categories: arXiv åˆ†ç±»åˆ—è¡¨
        last_scan: ä¸Šæ¬¡æ‰«ææ—¶é—´ (ISO æ ¼å¼)
        max_papers: æœ€å¤§è¿”å›æ•°é‡

    Returns:
        æ–°è®ºæ–‡åˆ—è¡¨
    """
    # è§£æ last_scan æ—¶é—´
    try:
        last_scan_dt = datetime.strptime(last_scan[:19], "%Y-%m-%dT%H:%M:%S")
        last_scan_timestamp = last_scan_dt.timestamp()
    except (ValueError, TypeError):
        last_scan_timestamp = 0

    all_papers = []

    for category in categories:
        print(f"ğŸ“¥ è·å– {category} åˆ†ç±»è®ºæ–‡...")

        # arXiv API æŸ¥è¯¢ URL
        base_url = "http://export.arxiv.org/api/query"
        query = f"cat:{category}"
        params = {
            "search_query": query,
            "sortBy": "submittedDate",
            "sortOrder": "descending",
            "max_results": max_papers * 3,  # å¤šå–ä¸€äº›ï¼Œå› ä¸ºæœ‰äº›å¯èƒ½è¿‡æœŸ
        }

        url = f"{base_url}?{urllib.parse.urlencode(params)}"
        print(f"   URL: {url}")

        try:
            response = feedparser.parse(url)

            if response.bozo:
                print(f"   âš ï¸  è§£æå¤±è´¥: {response.bozo_exception}")

            entry_count = 0
            for entry in response.entries:
                # è§£æå‘å¸ƒæ—¶é—´
                published_str = entry.get("published", "")
                try:
                    published_dt = datetime.strptime(published_str[:19], "%Y-%m-%dT%H:%M:%S")
                    published_timestamp = published_dt.timestamp()
                except (ValueError, TypeError):
                    published_timestamp = 0

                # è¿‡æ»¤ï¼šåªä¿ç•™ last_scan ä¹‹åçš„æ–°è®ºæ–‡
                if published_timestamp <= last_scan_timestamp:
                    continue

                # æå–ä½œè€…
                authors = [author.get("name", "") for author in entry.get("authors", [])]
                author_str = ", ".join(authors[:5])
                if len(authors) > 5:
                    author_str += f" ç­‰ {len(authors)} ä½ä½œè€…"

                # æ¸…ç†æ‘˜è¦
                summary = clean_text(entry.get("summary", ""))
                summary = truncate_text(summary)

                # æå–è®ºæ–‡ URL
                arxiv_url = ""
                for link in entry.get("links", []):
                    if link.get("type", "").startswith("text/html"):
                        arxiv_url = link.get("href", "")
                        break
                if not arxiv_url:
                    arxiv_url = f"https://arxiv.org/abs/{entry.get('id', '').split('/abs/')[-1]}"

                # æå– arXiv ID
                arxiv_id = entry.get("id", "").split("/abs/")[-1]

                paper = {
                    "id": arxiv_id,
                    "title": clean_text(entry.get("title", "")),
                    "summary": summary,
                    "url": arxiv_url,
                    "pdf_url": f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                    "authors": author_str,
                    "published": parse_arxiv_date(published_str),
                    "published_raw": published_str,
                    "category": category,
                    "tags": [tag.get("term", "") for tag in entry.get("tags", [])],
                }

                all_papers.append(paper)
                entry_count += 1

                if len(all_papers) >= max_papers:
                    break

            print(f"   âœ… è·å– {entry_count} ç¯‡æ–°è®ºæ–‡")

        except Exception as e:
            print(f"   âŒ è·å–å¤±è´¥: {e}")
            continue

    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆé™åºï¼‰
    all_papers.sort(key=lambda x: x.get("published_raw", ""), reverse=True)

    # å»é™¤é‡å¤ï¼ˆåŒä¸€è®ºæ–‡å¯èƒ½å‡ºç°åœ¨å¤šä¸ªåˆ†ç±»ï¼‰
    seen_ids = set()
    unique_papers = []
    for paper in all_papers:
        if paper["id"] not in seen_ids:
            seen_ids.add(paper["id"])
            unique_papers.append(paper)

    return unique_papers[:max_papers]


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="Monitor arXiv for new papers"
    )
    parser.add_argument(
        "--categories",
        type=str,
        default="cs.AI,cs.LG,cs.CL",
        help="arXiv categories (comma-separated)",
    )
    parser.add_argument(
        "--last-scan",
        type=str,
        default="",
        help="Last scan time (ISO format, e.g., 2026-01-01T00:00:00Z)",
    )
    parser.add_argument(
        "--max-papers",
        type=int,
        default=10,
        help="Maximum number of papers to return",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="/tmp/papers.json",
        help="Output JSON file path",
    )

    args = parser.parse_args(argv)

    # è§£æåˆ†ç±»åˆ—è¡¨
    categories = [c.strip() for c in args.categories.split(",") if c.strip()]

    # å¦‚æœæ²¡æœ‰æŒ‡å®š last-scanï¼Œé»˜è®¤ 7 å¤©å‰
    if not args.last_scan:
        last_scan = (datetime.now() - datetime.timedelta(days=7)).strftime(
            "%Y-%m-%dT%H:%M:%SZ"
        )
    else:
        last_scan = args.last_scan

    print(f"ğŸ” å¼€å§‹æ‰«æ arXiv...")
    print(f"   åˆ†ç±»: {', '.join(categories)}")
    print(f"   ä¸Šæ¬¡æ‰«æ: {last_scan}")
    print(f"   æœ€å¤§æ•°é‡: {args.max_papers}")
    print()

    # è·å–æ–°è®ºæ–‡
    papers = fetch_arxiv_papers(categories, last_scan, args.max_papers)

    print(f"\nğŸ“Š å…±æ‰¾åˆ° {len(papers)} ç¯‡æ–°è®ºæ–‡")

    # ä¿å­˜åˆ° JSON æ–‡ä»¶
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(papers, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

    # æ‰“å°æ‘˜è¦
    for i, paper in enumerate(papers, 1):
        print(f"   {i}. [{paper['category']}] {paper['title'][:50]}...")

    return 0


if __name__ == "__main__":
    sys.exit(main())
